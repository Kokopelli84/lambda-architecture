{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umsetzung der Batch-Schicht (Lambda-Architektur) mit S3, Redshift and Apache Kafka\n",
    "\n",
    "### Ziele:\n",
    "- Tweets-Datenstrom vom KafkaProducer in S3 übertragen\n",
    "- Batch-sicht berechnen (jede 24 Stunden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nötige Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "### KafkaConsumer-Konfiguration\n",
    "- Einen KafkaConsumer erzeugen, der Tweets vom KafkaProducer aufnimmt\n",
    "- Zu einem Kafka-Stream subscriben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "                        bootstrap_servers='localhost:9092',\n",
    "                        auto_offset_reset='latest',  # Reset partition offsets upon OffsetOutOfRangeError\n",
    "                        group_id='test',   # must have a unique consumer group id \n",
    "                        consumer_timeout_ms=1000)  \n",
    "                                # How long to listen for messages - we do it for 10 seconds \n",
    "                                # because we poll the kafka broker only each couple of hours\n",
    "\n",
    "consumer.subscribe('tweets-lambda1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "### Defining a Amazon Web Services S3 storage client\n",
    "- Authorisierungsinformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id='AKIAINL6QJLHMBXXFG7A',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "s3_client = s3_resource.meta.client\n",
    "bucket_name = 'tweets-lambda-architecture'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "### Rohdaten in S3 bucket speichern\n",
    "- Rohdaten jede Stunde in S3 speichern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_twitter_data(path):\n",
    "    csv_buffer = StringIO() # Buffer als String in S3 speichern\n",
    "\n",
    "    for message in consumer: # iteriert über die letzte Daten, die wir noch nicht gesehen haben\"\n",
    "        csv_buffer.write(message.value.decode() + '\\n') \n",
    "    s3_resource.Object(bucket_name,path).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "### Daten in Redshift exportieren und Batch-Sicht berechnen\n",
    "- mit Psycog in Redshift authentifizieren\n",
    "- letzte Daten aus S3 in Redshift für die Sichtberechnung exportieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "config = { 'dbname': 'lambda', \n",
    "           'user':'nelson',\n",
    "           'pwd':'LambdaApp1',\n",
    "           'host':'batch-view.ctgixhn76zcs.eu-central-1.redshift.amazonaws.com',\n",
    "           'port':'5439'\n",
    "         }\n",
    "conn =  psycopg2.connect(dbname=config['dbname'], host=config['host'], \n",
    "                              port=config['port'], user=config['user'], \n",
    "                              password=config['pwd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(conn, path):\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"\"\"\n",
    "        copy \n",
    "            batch_raw(id,created_at,location)\n",
    "        from \n",
    "            's3://tweets-lambda-architecture/\"\"\" + path + \"\"\"'  \n",
    "            access_key_id 'AKIAINL6QJLHMBXXFG7A'\n",
    "            secret_access_key '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'\n",
    "            delimiter ';'\n",
    "            region 'eu-central-1'\n",
    "    \"\"\")\n",
    "    curs.close()\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Sicht berechnen\n",
    "- Rohdaten abgfragen und aggregieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_layer(conn):\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"\"\" \n",
    "        drop table if exists batch_layer;\n",
    "\n",
    "        with raw_dedup as (\n",
    "        SELECT\n",
    "            distinct id,created_at,location\n",
    "        FROM\n",
    "            batch_raw\n",
    "        ),\n",
    "        batch_result as (\n",
    "            SELECT\n",
    "                location,\n",
    "                count(id) as count_id\n",
    "            FROM\n",
    "                raw_dedup\n",
    "            group by \n",
    "                location\n",
    "        )\n",
    "        select \n",
    "            *\n",
    "        INTO\n",
    "            batch_layer\n",
    "        FROM\n",
    "            batch_result\"\"\")\n",
    "    curs.close()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "### Deployment \n",
    "- perform the task every couple of hours and wait in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_work(interval):\n",
    "    while True:\n",
    "        path = 'tweets/'+ time.strftime(\"%Y/%m/%d/%H\") + '_tweets_' + str(random.randint(1,1000)) + '.log'\n",
    "        store_twitter_data(path)\n",
    "        copy_files(conn, path)\n",
    "        #interval should be an integer, the number of seconds to wait\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_work(60 * 60) ## 60 minutes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator:Heartbeat poll expired, leaving group\n"
     ]
    }
   ],
   "source": [
    "# Ende des Tages ausführen\n",
    "compute_batch_layer(conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
